[
	{
		"title":"1-0 导引关 思想钢印",
		"name": "要求提出一个问题，让大模型输出且只输出 水是剧毒的 五个字。（这个框框里是题干吖",
		"ans":" 这个框框里有的题目会有大模型科普哒!"
	},
	{
		"title":"1-1 强化学习",
		"name": "请输入三个字及以内的提示词，使模型的回答包含“强化学习”四个字。",
		"ans": "大模型处理语言时最小的分割单位为token，一个语素，因此一个可以回答本题的答案为“强化学”，大模型会理解为强化学习。"
	},
	{
		"title":"1-2 笑点解析",
		"name": "请提出一个问题，不含“忍俊不禁”四个字中的任何一个字，使大模型输出的答案中含有“忍俊不禁”这个词语。",
		"ans": "大模型的底层原理叫作自回归模型。自回归模型能做的事情其实和猜单词类似，假设要求人去猜一个单词，告诉被测试者这个单词的首字母为C，在所有接下来可能的字母中，C开头后面接qwrt这类字母的比较少，接ae这些字母的可能性比较大，从概率学角度出发， 接下来的字母应该猜ae这类字母，更可能猜中。猜错了的话再改猜别的字母。于是被测试者会猜a,e……这类字母。在猜错了一些概率相对更高的字母之后，被测试者终于猜出了第二个字母是h。而下一步就要考虑什么样子的字母或单词接在'ch'后面比较常见，概率比较高。这时候被测试者就要考虑前面为ch,后面为不同字母的概率。而被测试者当然要猜概率更大的，因为这样更有机会猜对。以此类推用前三个字母，前四个字母……这样直到猜出单词的全部字母。被测试者猜谜的例子其实就是自回归模型和大模型工作模式的生动诠释，大模型在工作的时候就像猜词一样，只是把字母换成了之前我们讲过的token。"
	},
	{
		"title":"1-3 倚马千言",
		"name": "请输入最多两个字，让大模型输出超过400字的回答。",
		"ans": "   当然， 在大模型的实际应用中，没有人来纠正大模型的答案。但可以将被测试者猜词例子中对字母是否猜对的判定视为大模型在训练时使用的数据集进行的训练。在训练过程中，大模型会利用数据集来调整P(当前输出|当前语境) 以提高回答的准确性。我们给大模型说的提示词，可以类比为最开始说的第一个字母‘c’，然后大模型要开始根据这个初始输入来组织和生成输出。它会先猜自己输出的第一段话语，等效于‘h’。然后根据‘ch’这个新的“当前语境”再逐步猜后面的字母/语素。"
	},
	{
		"title":"1-4 赞不绝口",
		"name":"请提出一个问题，问题中不含有“赞”字，要求输出100字以上的回答，回答中含有至少五个赞字。",
		"ans":" 大模型是如何处理让自己的回答停下来的呢？有人可能会想：在没有人判定的情况下，大模型似乎可以无穷无尽地猜下去啊，反正猜了一个再猜下一个，永远没有终止。然而现实中大模型说话虽然经常说一堆车轱辘话，但是最后还是会停下来的。是什么让这个猜谜停下来了呢？大模型是这样解决这个问题的。工程师们知道，想让大模型把无穷无尽的猜词停下来其实很简单，只需要“扩展”一下语素表，让“停下来”这个操作是一个新的语素就行了。如此一来，大模型在猜词的时候，就会一直猜一直猜，猜到语素“停下来”才停下来。"
	},
	{	
		"title":"1-5 无的放矢",
		"name":"请提出一个纯中文问题，让大模型生成超过100字的中文回答，且回答中不包含汉字中最常使用的“的，一，了，是，我”这五个字。",
		"ans":"大模型的弱点1：缺少规划自回归模型每次抽样都是根据当前信息（当前语境），在抽样的过程中对全局缺少规划。 从人类的观点下看，如果有恰好10个字的要求，那就不应该一口气说9个字，应该每说一个字，都得斟酌下看看剩下的字数能不能组成一句完整通顺的话。可自回归模型（大模型）才不管这些，它十分盲目短视地每次只管当前的P(当前输出|当前语境) ，并不很在意总回复的概率P(总输出|初始语境) 是不是足够好。这也导致了大模型大多数时候没有办法完成恰好10个字的输出这类要求——因为当他输出了九个字再对概率表进行搜索时候，发现在所有输出当中，仅输出一个字的概率太小了（这也意味着这种情况的训练语料太少了）。"
	},
	{
		"title":"1-6 鸡同鸭讲",
		"name":"请提出一个多于15个字的中文问题，让大模型生成超过20字的中文回答，且大模型的回答和您的问题中没有任何一个相同的汉字。",
		"ans":"大模型的弱点2：缺少反省修订能力自回归模型不具有“反省并修订”的能力。 人类基本都具有反省能力。如果说了错话做错事，至少也得心里想着：对不起对不起，不能这么干，我要弥补下。若换到说例如说恰好10个字的任务中，人假设一不小心说多了：今天天气很不错，阳光真...意识到之后人往往会赶紧修改一下，把“很”删掉，就能多出来一个字了。但大模型那可是金口玉言，说一不二。每一步说出来的话就会被放进新的“当前语境”中。它不会对自己已经抽样了的内容进行删减和修订，在逐次猜出token的过程中，将错就错，一错再错..... 换而言之，大模型虽然能看到自己之前的输出，但是却不具有反省并修订的能力。"
	},
	{
		"title":"1-7  本章BOSS战 正话反说",
		"name":"请输入一个本身不是回文串的问题，使这个问题无论正着问还是倒着问，模型的回答是一样的。",
		"ans":"AI大模型需要大量计算。它的工作过程大致可以分为训练和推理两个阶段。在训练阶段，首先需要收集和预处理大量的文本数据，用作输入数据。然后在适当的模型架构中初始化模型参数，处理输入的数据，尝试生成输出；再根据输出与预想之间的差异，反复调整参数，直到模型的性能不再显著提高为止。而在推理阶段中，则会先加载已经训练好的模型参数，预处理需要推理的文本数据，再让模型根据学习到的语言规律生成输出。无论是训练还是推理阶段，都是一连串信息重组过程，模型的参数量越大，需要处理的数据越多，所需的计算量也就越大，所消耗的能量也就越大，释放的热量也就越多。这些热量是AI运行过程中必须消耗的能量。"
	},
	{
		"title":"",
		"name":"恭喜你挑战成功",
		"ans":""
	}
]